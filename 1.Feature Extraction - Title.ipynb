{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### json 불러와서 저장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import jpype\n",
    "import glob\n",
    "from random import shuffle\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "data_path = \"data\"\n",
    "#data_path = \"data(test)\"\n",
    "\n",
    "file_list = glob.glob(\"%s/*.json\" % data_path)\n",
    "\n",
    "shuffle(file_list)\n",
    "\n",
    "json_train=[]\n",
    "\n",
    "for json_file_name in file_list:\n",
    "    json_file = json.loads(open(json_file_name).read())\n",
    "    json_train += json_file[\"articles\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### json에서 title 형태소 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c6a0b5562bb3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcnt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m     \u001b[0mtitle_pos\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"%s_%s\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmecab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticle\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"title\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     labeled_train.append({\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/konlpy/tag/_mecab.py\u001b[0m in \u001b[0;36mpos\u001b[0;34m(self, phrase, flatten)\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mflatten\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtagger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphrase\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.5/lib/python3.5/site-packages/MeCab.py\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[0m__getattr__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0m_swig_getattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTagger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[0m__repr__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_swig_repr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0m_MeCab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTagger_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparseToNode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0m_MeCab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTagger_parseToNode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparseNBest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mreturn\u001b[0m \u001b[0m_MeCab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTagger_parseNBest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Mecab\n",
    "\n",
    "mecab = Mecab()\n",
    "\n",
    "labeled_train = []\n",
    "\n",
    "for cnt, article in enumerate(json_train):\n",
    "    \n",
    "    if cnt % 10000 == 0:\n",
    "        print(cnt)\n",
    "        \n",
    "    title_pos = [\"%s_%s\" % (word, pos) for word, pos in mecab.pos(article[\"title\"])]\n",
    "\n",
    "    labeled_train.append({\n",
    "            \"istroll\": article[\"is_troll\"],\n",
    "            \"title_pos\": title_pos,\n",
    "            \"title_pos_sentences\" : \" \".join(title_pos),\n",
    "            \"pk\": article[\"pk\"]\n",
    "        })\n",
    "\n",
    "labeled_train = pd.DataFrame.from_dict(labeled_train)\n",
    "labeled_train = labeled_train.set_index('pk')\n",
    "\n",
    "labeled_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "used_model = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def make_bag_of_words(labeled_train, max_features, col_name):\n",
    "    global used_model\n",
    "    used_model = \"bow\"\n",
    "    \n",
    "    vectorizer = CountVectorizer(analyzer = \"word\", tokenizer = None, preprocessor = None,\n",
    "                                stop_words = None, max_features=max_features)\n",
    "    \n",
    "    train_data_features = vectorizer.fit_transform(labeled_train[col_name]).toarray()\n",
    "\n",
    "    col = [\"bow_%s_%s\" % (col_name, data) for data in vectorizer.get_feature_names()]\n",
    "    df_bow = pd.DataFrame(train_data_features, columns = col, index=labeled_train.index)\n",
    "    \n",
    "    labeled_train = pd.concat([labeled_train, df_bow],axis=1)\n",
    "    \n",
    "    return labeled_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora, models\n",
    "from gensim.corpora import TextCorpus, MmCorpus, Dictionary\n",
    "from nltk.corpus import stopwords\n",
    "from optparse import OptionParser\n",
    "\n",
    "def make_lda(train, keep_n, num_topics, col_name):\n",
    "    global used_model\n",
    "    used_model = \"lda\"\n",
    "\n",
    "    data = train[col_name]\n",
    "\n",
    "    dictionary = corpora.Dictionary(data)\n",
    "    dictionary.filter_extremes(keep_n=keep_n)\n",
    "\n",
    "    corpus = [dictionary.doc2bow(text) for text in data]\n",
    "\n",
    "    print(\"Make Lda..\")\n",
    "\n",
    "    lda = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, chunksize=1000, passes=1)\n",
    "    \n",
    "    num = len(train)    \n",
    "    df = []\n",
    "    \n",
    "    for i in range(0,num):\n",
    "        if i % 10000 == 0:\n",
    "            print(i)\n",
    "        \n",
    "        temp = [i[1] for i in lda.get_document_topics(corpus[i],minimum_probability=0)]\n",
    "        df.append(temp)\n",
    "    \n",
    "    col = [\"lda_%s_%d\" % (col_name, data) for data in range(0, num_topics)]\n",
    "    df = pd.DataFrame(df, columns = col)\n",
    "    df.index = train.index\n",
    "    \n",
    "    train = pd.concat([train, df], axis=1)\n",
    "        \n",
    "    return train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "def make_feature_vec(words, model, num_features):\n",
    "    \n",
    "    feature_vec = np.zeros((num_features,), dtype = \"float32\")\n",
    "    \n",
    "    nwords = 0\n",
    "    \n",
    "    index2word_set = set(model.index2word)\n",
    "    \n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            nwords = nwords + 1.\n",
    "            feature_vec = np.add(feature_vec, model[word])\n",
    "    \n",
    "    if nwords != 0:\n",
    "        feature_vec = np.divide(feature_vec, nwords)\n",
    "    \n",
    "    return feature_vec\n",
    "\n",
    "def get_avg_feature_vecs(texts, model, num_features):\n",
    "    \n",
    "    counter = 0\n",
    "    \n",
    "    text_feature_vecs = np.zeros((len(texts), num_features), dtype = \"float32\")\n",
    "    \n",
    "    for i, text in enumerate(texts):\n",
    "        \n",
    "        if i % 10000 == 0:\n",
    "            print(i)\n",
    "        \n",
    "        text_feature_vecs[i] = make_feature_vec(text , model, num_features)\n",
    "        \n",
    "    return text_feature_vecs\n",
    "\n",
    "def make_word2vec(train, col_name, max_features):\n",
    "    global used_model\n",
    "    used_model = \"word2vec\"\n",
    "    \n",
    "    num_features = max_features\n",
    "    min_word_count = 40\n",
    "    num_workers = -1\n",
    "    context = 10\n",
    "    downsampling = 1e-3\n",
    "    \n",
    "    sentences = \" \".join(train[col_name].apply(lambda x:\" \".join(x)))\n",
    "    \n",
    "    model = word2vec.Word2Vec(sentences, workers = num_workers, size = num_features,\\\n",
    "                             min_count = min_word_count, window = context, sample = downsampling)\n",
    "    \n",
    "    col = [\"word2vec_%s_%d\" % (col_name, data) for data in range(0, num_features)]\n",
    "    \n",
    "    train_feature = get_avg_feature_vecs(train[col_name].apply(lambda x:\" \".join(x)), model, num_features)\n",
    "    train_feature = pd.DataFrame(train_feature, index = train.index, columns = col)\n",
    "    \n",
    "    train = pd.concat([train, train_feature], axis = 1)\n",
    "    \n",
    "    return train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def make_tf_idf(train, max_features, col_name):\n",
    "    global used_model\n",
    "    used_model = \"tf-idf\"\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(min_df = 1, max_features = max_features)\n",
    "    \n",
    "    train_data_features = vectorizer.fit_transform(train[col_name]).toarray()\n",
    "    \n",
    "    col = [\"tfidf_%s_%s\" % (col_name, data) for data in vectorizer.get_feature_names()]\n",
    "    df_tfidf = pd.DataFrame(train_data_features, columns = col, index=train.index)\n",
    "    \n",
    "    train = pd.concat([train, df_tfidf],axis=1)\n",
    "    \n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_features = 1000\n",
    "#%time labeled_train = make_lda(labeled_train, 5000, 1000, \"title_pos\")\n",
    "#%time labeled_train = make_bag_of_words(labeled_train, 300, \"title_pos_sentences\")\n",
    "#labeled_train = make_tf_idf(labeled_train, max_features, \"title_pos_sentences\")\n",
    "labeled_train = make_word2vec(labeled_train,\"title_pos_sentences\",max_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predictor, model 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "label = 'istroll'\n",
    "pre = labeled_train.columns.drop(['title_pos', 'title_pos_sentences', label])\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=10, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import cross_validation\n",
    "\n",
    "cv_value = 3\n",
    "\n",
    "scores = cross_validation.cross_val_score(model, labeled_train[pre], labeled_train[label], cv=cv_value, scoring=\"roc_auc\")\n",
    "cv_result = scores.mean()\n",
    "\n",
    "print(cv_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result\n",
    "  * title의 morphs를 bag of words로 변환 (feature : 1,000개) - 0.612752193303\n",
    "  * title의 pos를 bag of words로 변환 (feature : 1,000개) - 0.630053006037\n",
    "  * 데이터를 셔플하고 title의 pos를 bag of words로 변환 (feature : 1,000개) - 0.658894903487\n",
    "  * 데이터를 셔플하고 title의 pos를 tf-idf로 변환 (feature : 1,000개) - 0.648875263729\n",
    "  \n",
    "  * title의 pos를 LDA로 변환 (keep_n : 5,000, num_topics : 1,000) - 0.592446285012"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Accuracy와 Cross Validation Accuracy 측정 후 그래프 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def calcul_score(labeled_train):\n",
    "    label = 'istroll'\n",
    "    predictors = labeled_train.columns.drop([label, 'author_pos', 'author_pos_sentences', 'author'])\n",
    "\n",
    "    alg = RandomForestClassifier(n_estimators = 10, n_jobs = -1)\n",
    "    kf = KFold(len(labeled_train), n_folds=3, shuffle=True)\n",
    "    \n",
    "    train_score = []\n",
    "    cv_score = []\n",
    "    for train_index, test_index in kf:\n",
    "        alg.fit(labeled_train.loc[labeled_train.index[train_index], predictors], labeled_train.loc[labeled_train.index[train_index], label])\n",
    "        \n",
    "        train_predicted = alg.predict_proba(labeled_train.loc[labeled_train.index[train_index], predictors])\n",
    "        train_score.append(roc_auc_score(labeled_train.loc[labeled_train.index[train_index], label], train_predicted.T[1]))\n",
    "        \n",
    "        cv_predicted = alg.predict_proba(labeled_train.loc[labeled_train.index[test_index], predictors])\n",
    "        cv_score.append(roc_auc_score(labeled_train.loc[labeled_train.index[test_index], label], cv_predicted.T[1]))\n",
    "    \n",
    "    train_score = np.mean(train_score)\n",
    "    cv_score = np.mean(cv_score)\n",
    "    \n",
    "    print(\"train score = %s\\ntest score = %s\" % (train_score, cv_score))\n",
    "    \n",
    "    return [train_score, cv_score]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "result = []\n",
    "for i in range(1, 11):\n",
    "    # %time bow_labeled_train = make_bag_of_words(labeled_train, 100 * i, \"author_pos_sentences\")\n",
    "    %time labeled_train = make_lda(labeled_train, 1000, 100 * i, \"author_pos\")\n",
    "    # %time labeled_train = make_word2vec(labeled_train, \"author_pos\", 100 * i)\n",
    "    result.append(calcul_score(bow_labeled_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "plt.ioff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = [x * 100 for x in range(1, 11)]\n",
    "\n",
    "plt.plot(x, np.array(result).T[0], label=\"train\")\n",
    "plt.plot(x, np.array(result).T[1], label=\"cross validation\")\n",
    "plt.legend(loc=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pickle로 파일 저장"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "if not os.path.exists(used_model):\n",
    "    os.makedirs(used_model)\n",
    "\n",
    "pickle.dump(labeled_train[pre], open(\"%s/title_%d.p\" % (used_model, max_features), \"wb\"), protocol = pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
