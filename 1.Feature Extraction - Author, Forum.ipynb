{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### json 불러와서 저장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import jpype\n",
    "import glob\n",
    "from random import shuffle\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "#data_path = \"data\"\n",
    "data_path = \"data(test)\"\n",
    "\n",
    "file_list = glob.glob(\"%s/*.json\" % data_path)\n",
    "\n",
    "json_train=[]\n",
    "\n",
    "shuffle(file_list)\n",
    "for json_file_name in file_list:\n",
    "    json_file = json.loads(open(json_file_name).read())\n",
    "    json_train += json_file[\"articles\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### json에서 author, author 형태소, forumid 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "\n",
    "mecab = Mecab()\n",
    "\n",
    "labeled_train = []\n",
    "\n",
    "for cnt, article in enumerate(json_train):\n",
    "    if cnt % 10000 == 0:\n",
    "        print(cnt)\n",
    "    author_pos = [\"%s_%s\" % (word, pos) for word, pos in mecab.pos(article[\"author\"])]\n",
    "\n",
    "    labeled_train.append({\n",
    "            \"istroll\": article[\"is_troll\"],\n",
    "            \"author\": article[\"author\"],\n",
    "            \"author_pos\": author_pos,\n",
    "            \"author_pos_sentences\" : \" \".join(author_pos),\n",
    "            \"forumid\": article[\"forumid\"],\n",
    "            \"pk\": article[\"pk\"]\n",
    "        })\n",
    "\n",
    "labeled_train = pd.DataFrame.from_dict(labeled_train)\n",
    "labeled_train = labeled_train.set_index('pk')\n",
    "\n",
    "labeled_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### forumid와 author을 label encoder로 숫자로 바꿔줌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "le = preprocessing.LabelEncoder()\n",
    "\n",
    "labeled_train[\"forumid\"] = le.fit_transform(labeled_train[\"forumid\"])\n",
    "labeled_train[\"author\"] = le.fit_transform(labeled_train[\"author\"])\n",
    "\n",
    "labeled_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "used_model = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def make_bag_of_words(labeled_train, max_features, col_name):\n",
    "    global used_model\n",
    "    used_model = \"bow\"\n",
    "    \n",
    "    vectorizer = CountVectorizer(analyzer = \"word\", tokenizer = None, preprocessor = None,\n",
    "                                stop_words = None, max_features=max_features)\n",
    "    \n",
    "    train_data_features = vectorizer.fit_transform(labeled_train[col_name]).toarray()\n",
    "\n",
    "    col = [\"bow_%s_%s\" % (col_name, data) for data in vectorizer.get_feature_names()]\n",
    "    df_bow = pd.DataFrame(train_data_features, columns = col, index=labeled_train.index)\n",
    "    \n",
    "    labeled_train = pd.concat([labeled_train, df_bow],axis=1)\n",
    "    \n",
    "    return labeled_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora, models\n",
    "from gensim.corpora import TextCorpus, MmCorpus, Dictionary\n",
    "from nltk.corpus import stopwords\n",
    "from optparse import OptionParser\n",
    "\n",
    "def make_lda(train, keep_n, num_topics, col_name):\n",
    "    global used_model\n",
    "    used_model = \"lda\"\n",
    "\n",
    "    data = train[col_name]\n",
    "\n",
    "    dictionary = corpora.Dictionary(data)\n",
    "    dictionary.filter_extremes(keep_n=keep_n)\n",
    "\n",
    "    corpus = [dictionary.doc2bow(text) for text in data]\n",
    "\n",
    "    print(\"Make Lda..\")\n",
    "\n",
    "    lda = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, chunksize=1000, passes=1)\n",
    "    \n",
    "    num = len(train)    \n",
    "    df = []\n",
    "    \n",
    "    for i in range(0,num):\n",
    "        if i % 10000 == 0:\n",
    "            print(i)\n",
    "        \n",
    "        temp = [i[1] for i in lda.get_document_topics(corpus[i],minimum_probability=0)]\n",
    "        df.append(temp)\n",
    "    \n",
    "    col = [\"lda_%s_%d\" % (col_name, data) for data in range(0, num_topics)]\n",
    "    df = pd.DataFrame(df, columns = col)\n",
    "    df.index = train.index\n",
    "    \n",
    "    train = pd.concat([train, df], axis=1)\n",
    "        \n",
    "    return train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "def make_feature_vec(words, model, num_features):\n",
    "    \n",
    "    feature_vec = np.zeros((num_features,), dtype = \"float32\")\n",
    "    \n",
    "    nwords = 0\n",
    "    \n",
    "    index2word_set = set(model.index2word)\n",
    "    \n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            nwords = nwords + 1.\n",
    "            feature_vec = np.add(feature_vec, model[word])\n",
    "    \n",
    "    if nwords != 0:\n",
    "        feature_vec = np.divide(feature_vec, nwords)\n",
    "    \n",
    "    return feature_vec\n",
    "\n",
    "def get_avg_feature_vecs(texts, model, num_features):\n",
    "    \n",
    "    counter = 0\n",
    "    \n",
    "    text_feature_vecs = np.zeros((len(texts), num_features), dtype = \"float32\")\n",
    "    \n",
    "    for i, text in enumerate(texts):\n",
    "        \n",
    "        if i % 10000 == 0:\n",
    "            print(i)\n",
    "        \n",
    "        text_feature_vecs[i] = make_feature_vec(text , model, num_features)\n",
    "        \n",
    "    return text_feature_vecs\n",
    "\n",
    "def make_word2vec(train, col_name, max_features):\n",
    "    global used_model\n",
    "    used_model = \"word2vec\"\n",
    "    \n",
    "    num_features = max_features\n",
    "    min_word_count = 40\n",
    "    num_workers = -1\n",
    "    context = 10\n",
    "    downsampling = 1e-3\n",
    "    \n",
    "    sentences = \" \".join(train[col_name].apply(lambda x:\" \".join(x)))\n",
    "    \n",
    "    model = word2vec.Word2Vec(sentences, workers = num_workers, size = num_features,\\\n",
    "                             min_count = min_word_count, window = context, sample = downsampling)\n",
    "    \n",
    "    col = [\"word2vec_%s_%d\" % (col_name, data) for data in range(0, num_features)]\n",
    "    \n",
    "    train_feature = get_avg_feature_vecs(train[col_name].apply(lambda x:\" \".join(x)), model, num_features)\n",
    "    train_feature = pd.DataFrame(train_feature, index = train.index, columns = col)\n",
    "    \n",
    "    train = pd.concat([train, train_feature], axis = 1)\n",
    "    \n",
    "    return train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def make_tf_idf(train, max_features, col_name):\n",
    "    global used_model\n",
    "    used_model = \"tf-idf\"\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(min_df = 1, max_features = max_features)\n",
    "    \n",
    "    train_data_features = vectorizer.fit_transform(train[col_name]).toarray()\n",
    "    \n",
    "    col = [\"tfidf_%s_%s\" % (col_name, data) for data in vectorizer.get_feature_names()]\n",
    "    df_tfidf = pd.DataFrame(train_data_features, columns = col, index=train.index)\n",
    "    \n",
    "    train = pd.concat([train, df_tfidf],axis=1)\n",
    "    \n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_features = 600\n",
    "#%time labeled_train = make_lda(labeled_train, 1000, 200, \"author_pos\")\n",
    "#%time labeled_train = make_bag_of_words(labeled_train, max_features, \"author_pos_sentences\")\n",
    "labeled_train = make_word2vec(labeled_train, \"author_pos\", max_features)\n",
    "#labeled_train = make_tf_idf(labeled_train, max_features, \"author_pos_sentences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predictor, model 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "label = 'istroll'\n",
    "pre = labeled_train.columns.drop(['author_pos', 'author_pos_sentences', 'author', label])\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=10, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import cross_validation\n",
    "\n",
    "cv_value = 3\n",
    "\n",
    "scores = cross_validation.cross_val_score(model, labeled_train[pre], labeled_train[label], cv=cv_value, scoring=\"roc_auc\")\n",
    "\n",
    "cv_result = scores.mean()\n",
    "\n",
    "print(cv_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result\n",
    "\n",
    " * author를 Label Encoder로 변환: 0.67331271058\n",
    " * author의 morphs를 Bag of Words(feature = 1,000 개)로 변환: 0.733333689652\n",
    " * author의 pos를 Bag of Words(feature = 1,000 개)로 변환: 0.769833271386\n",
    " * 데이터를 shuffle하고 pos를 Bag of Words(feautre = 1,000개)로 변환: 0.81172740086\n",
    " * 데이터를 shuffle하고 pos를 TF-IDF(feature = 1,000개)로 변환 : 0.804858083797\n",
    " \n",
    " \n",
    " * author의 pos를 LDA로 변환 (num_topics = 200, keep_n = 1,000) : 0.748718319834\n",
    " * 데이터를 shuffle하고 author의 pos를 LDA로 변환 (num_topics = 200, keep_n = 1,000) : 0.772313052394"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "pre = labeled_train.columns.drop(['author_pos', 'author_pos_sentences', 'author'])\n",
    "\n",
    "if not os.path.exists(used_model):\n",
    "    os.makedirs(used_model)\n",
    "\n",
    "pickle.dump(labeled_train[pre], open(\"%s/author, forum_%d.p\" % (used_model, max_features), \"wb\"), protocol = pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
