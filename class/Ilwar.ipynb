{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def make_bag_of_words(labeled_train, max_features, col_name):\n",
    "    global used_model\n",
    "    used_model = \"bow\"\n",
    "    \n",
    "    vectorizer = CountVectorizer(analyzer = \"word\", tokenizer = None, preprocessor = None,\n",
    "                                stop_words = None, max_features=max_features)\n",
    "    \n",
    "    train_data_features = vectorizer.fit_transform(labeled_train[col_name]).toarray()\n",
    "\n",
    "    col = [\"bow_%s_%s\" % (col_name, data) for data in vectorizer.get_feature_names()]\n",
    "    df_bow = pd.DataFrame(train_data_features, columns = col, index=labeled_train.index)\n",
    "    \n",
    "    labeled_train = pd.concat([labeled_train, df_bow],axis=1)\n",
    "    \n",
    "    return labeled_train\n",
    "\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "from gensim.models import word2vec\n",
    "\n",
    "def make_feature_vec(words, model, num_features):\n",
    "    \n",
    "    feature_vec = np.zeros((num_features,), dtype = \"float32\")\n",
    "    \n",
    "    nwords = 0\n",
    "    \n",
    "    index2word_set = set(model.index2word)\n",
    "    \n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            nwords = nwords + 1.\n",
    "            feature_vec = np.add(feature_vec, model[word])\n",
    "    \n",
    "    if nwords != 0:\n",
    "        feature_vec = np.divide(feature_vec, nwords)\n",
    "    \n",
    "    return feature_vec\n",
    "\n",
    "def get_avg_feature_vecs(texts, model, num_features):\n",
    "    \n",
    "    counter = 0\n",
    "    \n",
    "    text_feature_vecs = np.zeros((len(texts), num_features), dtype = \"float32\")\n",
    "    \n",
    "    for i, text in enumerate(texts):\n",
    "        \n",
    "        if i % 10000 == 0:\n",
    "            print(i)\n",
    "        \n",
    "        text_feature_vecs[i] = make_feature_vec(text , model, num_features)\n",
    "        \n",
    "    return text_feature_vecs\n",
    "\n",
    "def make_word2vec(train, col_name, max_features):\n",
    "    global used_model\n",
    "    used_model = \"word2vec\"\n",
    "    \n",
    "    num_features = max_features\n",
    "    min_word_count = 40\n",
    "    num_workers = -1\n",
    "    context = 10\n",
    "    downsampling = 1e-3\n",
    "    \n",
    "    sentences = \" \".join(train[col_name].apply(lambda x:\" \".join(x)))\n",
    "    \n",
    "    model = word2vec.Word2Vec(sentences, workers = num_workers, size = num_features,                             min_count = min_word_count, window = context, sample = downsampling)\n",
    "    \n",
    "    col = [\"word2vec_%s_%d\" % (col_name, data) for data in range(0, num_features)]\n",
    "    \n",
    "    train_feature = get_avg_feature_vecs(train[col_name].apply(lambda x:\" \".join(x)), model, num_features)\n",
    "    train_feature = pd.DataFrame(train_feature, index = train.index, columns = col)\n",
    "    \n",
    "    train = pd.concat([train, train_feature], axis = 1)\n",
    "    \n",
    "    return train\n",
    "\n",
    "\n",
    "# In[6]:\n",
    "\n",
    "from konlpy.tag import Mecab\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import jpype\n",
    "import glob\n",
    "from random import shuffle\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from sklearn import preprocessing\n",
    "\n",
    "class Ilwar:\n",
    "    def __init__(self, path):\n",
    "        self.train_path = path\n",
    "    def set_train_path(self, path):\n",
    "        self.train_path = path\n",
    "    def fit(self):\n",
    "        file_list = glob.glob(\"%s/*.json\" % self.train_path)\n",
    "        shuffle(file_list)\n",
    "        json_train=[]\n",
    "\n",
    "        for json_file_name in file_list:\n",
    "            json_file = json.loads(open(json_file_name).read())\n",
    "            json_train += json_file[\"articles\"]\n",
    "    \n",
    "        mecab = Mecab()\n",
    "\n",
    "        labeled_train = []\n",
    "\n",
    "        for cnt, article in enumerate(json_train):\n",
    "            if cnt % 10000 == 0:\n",
    "                print(cnt)\n",
    "                \n",
    "            text = bs(article[\"text\"], \"html.parser\").text\n",
    "            title_pos = [\"%s_%s\" % (word, pos) for word, pos in mecab.pos(article[\"title\"])]\n",
    "            author_pos = [\"%s_%s\" % (word, pos) for word, pos in mecab.pos(article[\"author\"])]\n",
    "            text_pos = [\"%s_%s\" % (first, second) for first, second in mecab.pos(text)]\n",
    "\n",
    "            labeled_train.append({\n",
    "                \"istroll\": article[\"is_troll\"],\n",
    "                \"title_pos\": title_pos,\n",
    "                \"title_pos_sentences\" : \" \".join(title_pos),\n",
    "                \"author_pos\": author_pos,\n",
    "                \"author_pos_sentences\" : \" \".join(author_pos),\n",
    "                \"text_pos\": text_pos,\n",
    "                \"text_pos_sentences\" : \" \".join(text_pos),\n",
    "                \"forumid\": article[\"forumid\"],                    \n",
    "                \"pk\": article[\"pk\"]\n",
    "            })\n",
    "\n",
    "        labeled_train = pd.DataFrame.from_dict(labeled_train)\n",
    "        labeled_train = labeled_train.set_index('pk')\n",
    "        \n",
    "        labeled_train = make_word2vec(labeled_train, \"author_pos\", 600)\n",
    "        labeled_train = make_bag_of_words(labeled_train, 500, \"title_pos_sentences\")\n",
    "        labeled_train = make_bag_of_words(labeled_train, 500, \"text_pos_sentences\")\n",
    "\n",
    "        le = preprocessing.LabelEncoder()\n",
    "\n",
    "        labeled_train[\"forumid\"] = le.fit_transform(labeled_train[\"forumid\"])\n",
    "        \n",
    "        label = 'istroll'\n",
    "        pre = labeled_train.columns.drop(['author_pos', 'author_pos_sentences'                                          ,'title_pos', 'title_pos_sentences',                                          'text_pos', 'text_pos_sentences', label])\n",
    "        \n",
    "        self.model = RandomForestClassifier(n_estimators=10, n_jobs=-1)\n",
    "        self.model.fit(labeled_train[pre],labeled_train[label])\n",
    "        \n",
    "        print(\"fit complete\")\n",
    "        \n",
    "    def predict_file(self, path):\n",
    "        json_file = json.loads(open(path).read())\n",
    "        json_test = json_file[\"articles\"]\n",
    "        \n",
    "        mecab = Mecab()\n",
    "        test = []\n",
    "        article = json_test[0]\n",
    "\n",
    "        text = bs(article[\"text\"], \"html.parser\").text\n",
    "        title_pos = [\"%s_%s\" % (word, pos) for word, pos in mecab.pos(article[\"title\"])]\n",
    "        author_pos = [\"%s_%s\" % (word, pos) for word, pos in mecab.pos(article[\"author\"])]\n",
    "        text_pos = [\"%s_%s\" % (first, second) for first, second in mecab.pos(text)]\n",
    "        \n",
    "        test.append({\n",
    "            \"title_pos\": title_pos,\n",
    "            \"title_pos_sentences\" : \" \".join(title_pos),\n",
    "            \"author_pos\": author_pos,\n",
    "            \"author_pos_sentences\" : \" \".join(author_pos),\n",
    "            \"text_pos\": text_pos,\n",
    "            \"text_pos_sentences\" : \" \".join(text_pos),\n",
    "            \"forumid\": article[\"forumid\"],                    \n",
    "            \"pk\": article[\"pk\"]\n",
    "        })\n",
    "        \n",
    "        test = pd.DataFrame(test)\n",
    "        test = test.set_index(\"pk\")\n",
    "\n",
    "        le = preprocessing.LabelEncoder()\n",
    "\n",
    "        test[\"forumid\"] = le.fit_transform(test[\"forumid\"])\n",
    "\n",
    "        pre = test.columns.drop(['author_pos', 'author_pos_sentences','title_pos', 'title_pos_sentences','text_pos', 'text_pos_sentences'])\n",
    "        \n",
    "        result = self.model.predict_proba(test[pre])\n",
    "        \n",
    "        return result\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
