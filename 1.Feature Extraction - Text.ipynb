{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### json 불러와서 저장하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import jpype\n",
    "import glob\n",
    "from random import shuffle\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "data_path = \"data\"\n",
    "#data_path = \"data(test)\"\n",
    "\n",
    "file_list = glob.glob(\"%s/*.json\" % data_path)\n",
    "shuffle(file_list)\n",
    "\n",
    "json_train=[]\n",
    "for json_file_name in file_list:\n",
    "    json_file = json.loads(open(json_file_name).read())\n",
    "    json_train += json_file[\"articles\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### json에서 text 형태소 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from konlpy.tag import Mecab\n",
    "\n",
    "mecab = Mecab()\n",
    "\n",
    "labeled_train = []\n",
    "\n",
    "for cnt, article in enumerate(json_train):\n",
    "    text = bs(article[\"text\"], \"html.parser\").text\n",
    "    \n",
    "    if cnt % 10000 == 0:\n",
    "        print(cnt)\n",
    "        \n",
    "    text_pos = [\"%s_%s\" % (first, second) for first, second in mecab.pos(text)]\n",
    "\n",
    "    labeled_train.append({\n",
    "            \"istroll\": article[\"is_troll\"],\n",
    "            \"text_pos\": text_pos,\n",
    "            \"text_pos_sentences\" : \" \".join(text_pos),\n",
    "            \"pk\": article[\"pk\"]\n",
    "        })\n",
    "\n",
    "labeled_train = pd.DataFrame.from_dict(labeled_train)\n",
    "labeled_train = labeled_train.set_index('pk')\n",
    "\n",
    "labeled_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "used_model = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bag of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def make_bag_of_words(labeled_train, max_features, col_name):\n",
    "    global used_model\n",
    "    used_model = \"bow\"\n",
    "    \n",
    "    vectorizer = CountVectorizer(analyzer = \"word\", tokenizer = None, preprocessor = None,\n",
    "                                stop_words = None, max_features=max_features)\n",
    "    \n",
    "    train_data_features = vectorizer.fit_transform(labeled_train[col_name]).toarray()\n",
    "\n",
    "    col = [\"bow_%s_%s\" % (col_name, data) for data in vectorizer.get_feature_names()]\n",
    "    df_bow = pd.DataFrame(train_data_features, columns = col, index=labeled_train.index)\n",
    "    \n",
    "    labeled_train = pd.concat([labeled_train, df_bow],axis=1)\n",
    "    \n",
    "    return labeled_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora, models\n",
    "from gensim.corpora import TextCorpus, MmCorpus, Dictionary\n",
    "from nltk.corpus import stopwords\n",
    "from optparse import OptionParser\n",
    "\n",
    "def make_lda(train, keep_n, num_topics, col_name):\n",
    "    global used_model\n",
    "    used_model = \"lda\"\n",
    "\n",
    "    data = train[col_name]\n",
    "\n",
    "    dictionary = corpora.Dictionary(data)\n",
    "    dictionary.filter_extremes(keep_n=keep_n)\n",
    "\n",
    "    corpus = [dictionary.doc2bow(text) for text in data]\n",
    "\n",
    "    print(\"Make Lda..\")\n",
    "\n",
    "    lda = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, chunksize=1000, passes=1)\n",
    "    \n",
    "    num = len(train)    \n",
    "    df = []\n",
    "    \n",
    "    for i in range(0,num):\n",
    "        if i % 10000 == 0:\n",
    "            print(i)\n",
    "        \n",
    "        temp = [i[1] for i in lda.get_document_topics(corpus[i],minimum_probability=0)]\n",
    "        df.append(temp)\n",
    "    \n",
    "    col = [\"lda_%s_%d\" % (col_name, data) for data in range(0, num_topics)]\n",
    "    df = pd.DataFrame(df, columns = col)\n",
    "    df.index = train.index\n",
    "    \n",
    "    train = pd.concat([train, df], axis=1)\n",
    "        \n",
    "    return train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def make_tf_idf(train, max_features, col_name):\n",
    "    global used_model\n",
    "    used_model = \"tf-idf\"\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(min_df = 1, max_features = max_features)\n",
    "    \n",
    "    train_data_features = vectorizer.fit_transform(train[col_name]).toarray()\n",
    "    \n",
    "    col = [\"tfidf_%s_%s\" % (col_name, data) for data in vectorizer.get_feature_names()]\n",
    "    df_tfidf = pd.DataFrame(train_data_features, columns = col, index=train.index)\n",
    "    \n",
    "    train = pd.concat([train, df_tfidf],axis=1)\n",
    "    \n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "max_features = 1000\n",
    "#%time labeled_train = make_lda(labeled_train, 5000, 1000, \"text_pos\")\n",
    "#%time labeled_train = make_bag_of_words(labeled_train, 1000, \"text_pos_sentences\")\n",
    "labeled_train = make_tf_idf(labeled_train, max_features, \"text_pos_sentences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### predictor, model 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "label = 'istroll'\n",
    "pre = labeled_train.columns.drop(['text_pos', 'text_pos_sentences', label])\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=10, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import cross_validation\n",
    "\n",
    "cv_value = 3\n",
    "\n",
    "scores = cross_validation.cross_val_score(model, labeled_train[pre], labeled_train[label], cv=cv_value, scoring=\"roc_auc\")\n",
    "cv_result = scores.mean()\n",
    "\n",
    "print(cv_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result\n",
    "  * text의 morphs를 bag of words로 변환 (feature : 1,000개) - 0.637526911627\n",
    "  * text의 pos를 bag of words로 변환 (feature : 1,000개) - 0.661265083065\n",
    "  * 데이터를 셔플하고 text의 pos를 bag of words로 변환 (feature : 1,000개) - 0.688875771784\n",
    "  \n",
    "  * 데이터를 셔플하고 text의 pos를 tf-idf로 변환 (feature : 1,000개) - 0.683697553571\n",
    "  \n",
    "  * text의 pos를 lda로 변환 (keep_n : 5,000개, num_topics : 1,000개) - 0.643143549702\n",
    "  * bs의 파라미터를 \"lxml\"이 아닌 \"html.parser\"로 주고 pos를 bag of words로 변환 (feature : 1,000개) - 0.657726939833"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "if not os.path.exists(used_model):\n",
    "    os.makedirs(used_model)\n",
    "\n",
    "pickle.dump(labeled_train[pre], open(\"%s/text_%d.p\" % (used_model, max_features), \"wb\"), protocol = pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
